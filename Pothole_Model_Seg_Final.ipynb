{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, glob, pickle, json, time\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kOQJbFRTq9cT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 모델 정의 —\n",
        "def get_deeplab_model(num_classes: int) -> nn.Module:\n",
        "    model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
        "    model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "    return model\n",
        "\n",
        "class CBAMBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels//reduction, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(channels//reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg = self.fc(self.avg_pool(x))\n",
        "        maxv = self.fc(self.max_pool(x))\n",
        "        attn = self.sigmoid(avg + maxv)\n",
        "        return x * attn\n",
        "\n",
        "class ResNetCBAM(nn.Module):\n",
        "    def __init__(self, num_classes: int = 3):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "        self.backbone.layer1.add_module('cbam1', CBAMBlock(256))\n",
        "        self.backbone.layer2.add_module('cbam2', CBAMBlock(512))\n",
        "        in_feat = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_feat, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ],
      "metadata": {
        "id": "zNhxVug5pgmL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 유틸리티 함수 —\n",
        "def load_annotations(path, retries=3, delay=1):\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            with open(path,'r',encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except OSError:\n",
        "            time.sleep(delay)\n",
        "    raise\n",
        "\n",
        "def create_mask(bboxes, shape):\n",
        "    img = Image.new('L', (shape[1], shape[0]), 0)\n",
        "    d = ImageDraw.Draw(img)\n",
        "    for x,y,w,h in bboxes:\n",
        "        d.rectangle([x,y,x+w,y+h], fill=1)\n",
        "    return np.array(img, dtype=np.int64)\n",
        "\n",
        "def warp_roi(img_t, mask_t, bboxes, size=(224,224)):\n",
        "    mean,std = img_t.new_tensor([0.485,0.456,0.406]).view(3,1,1), img_t.new_tensor([0.229,0.224,0.225]).view(3,1,1)\n",
        "    img = (img_t*std+mean)*255\n",
        "    np_img = img.clamp(0,255).byte().cpu().permute(1,2,0).numpy()[...,::-1]\n",
        "    m = (mask_t.cpu().numpy()>0).astype(np.uint8)\n",
        "    cnts,_ = cv2.findContours(m,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not cnts: return None,[]\n",
        "    c = max(cnts, key=cv2.contourArea)\n",
        "    pts = cv2.approxPolyDP(c,0.02*cv2.arcLength(c,True),True).reshape(-1,2) \\\n",
        "          if len(c)>=4 else cv2.boxPoints(cv2.minAreaRect(c))\n",
        "    s,d = pts.sum(1), np.diff(pts,axis=1).ravel()\n",
        "    tl,br,tr,bl = pts[np.argmin(s)], pts[np.argmax(s)], pts[np.argmin(d)], pts[np.argmax(d)]\n",
        "    src = np.array([tl,tr,br,bl], float)\n",
        "    dst = np.array([[0,0],[size[0]-1,0],[size[0]-1,size[1]-1],[0,size[1]-1]], float)\n",
        "    H = cv2.getPerspectiveTransform(src, dst)\n",
        "    wimg = cv2.warpPerspective(np_img, H, size)\n",
        "    roi = torch.from_numpy(wimg[...,::-1]).permute(2,0,1).float().to(img_t.device)/255\n",
        "    return (roi-mean)/std, []"
      ],
      "metadata": {
        "id": "XdL4-umCq7UX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 경로 설정 및 어노테이션 로드 —\n",
        "DATA_ROOT = '/content/drive/MyDrive/pt_data'\n",
        "AIHUB_ANN = os.path.join(DATA_ROOT, 'aihub_annotations.json')\n",
        "RDD_ANN   = os.path.join(DATA_ROOT, 'rdd2022_train_annotations.json')\n",
        "ann_dict  = {**load_annotations(AIHUB_ANN), **load_annotations(RDD_ANN)}\n",
        "\n",
        "# — 파일 목록 준비 —\n",
        "train_pkls = glob.glob(os.path.join(DATA_ROOT, 'AIhub_Road',    'training_image_batch_*.pkl')) + \\\n",
        "             glob.glob(os.path.join(DATA_ROOT, 'RDD2022',        'training_image_batch_*.pkl'))\n",
        "val_pkls   = glob.glob(os.path.join(DATA_ROOT, 'AIhub_Road',    'validation_image_batch_*.pkl')) + \\\n",
        "             glob.glob(os.path.join(DATA_ROOT, 'RDD2022',        'validation_image_batch_*.pkl'))"
      ],
      "metadata": {
        "id": "-q8qw0Dgq4BB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 트랜스폼 정의 —\n",
        "seg_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "roi_tf = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "1PieIAm9q1eA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 훈련 함수 —\n",
        "def train_seg(seg_epochs=5, seg_bs=16):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    scaler_s = GradScaler()\n",
        "\n",
        "    seg_model = get_deeplab_model(2).to(device)\n",
        "    seg_opt   = torch.optim.Adam(seg_model.parameters(), lr=1e-4)\n",
        "    seg_loss  = nn.CrossEntropyLoss()\n",
        "\n",
        "    # — Segmentation loop —\n",
        "    for epoch in range(1, seg_epochs+1):\n",
        "        # train\n",
        "        seg_model.train()\n",
        "        total_loss, steps = 0, 0\n",
        "        for pklf in tqdm(train_pkls, desc=f\"Seg Train Ep{epoch}\", unit=\"file\"):\n",
        "            with open(pklf,'rb') as f: batch = pickle.load(f)\n",
        "            items = batch.items() if isinstance(batch,dict) else [(i,e['image']) for i,e in enumerate(batch)]\n",
        "            imgs, msks = [], []\n",
        "            for key, img in items:\n",
        "                arr = img.numpy() if hasattr(img,'numpy') else np.array(img)\n",
        "                imgs.append(seg_tf(Image.fromarray(arr.astype(np.uint8))))\n",
        "                mask = create_mask(ann_dict.get(key,{}).get('bboxes',[]), arr.shape[:2])\n",
        "                msks.append(torch.from_numpy(mask).long())\n",
        "            loader = DataLoader(TensorDataset(torch.stack(imgs), torch.stack(msks)),\n",
        "                                batch_size=seg_bs, shuffle=True,\n",
        "                                num_workers=4, pin_memory=True)\n",
        "            for x,y in loader:\n",
        "                x,y = x.to(device), y.to(device)\n",
        "                # Change autocast initialization for older PyTorch versions\n",
        "                #with autocast(enabled=device.type=='cuda'):\n",
        "                with autocast():\n",
        "                    out = seg_model(x)['out']\n",
        "                    loss = seg_loss(out, y)\n",
        "                seg_opt.zero_grad()\n",
        "                scaler_s.scale(loss).backward()\n",
        "                scaler_s.step(seg_opt)\n",
        "                scaler_s.update()\n",
        "                total_loss += loss.item(); steps += 1\n",
        "        avg_tr = total_loss/steps\n",
        "\n",
        "        # val\n",
        "        seg_model.eval()\n",
        "        v_loss, v_steps = 0, 0\n",
        "        for pklf in tqdm(val_pkls, desc=f\"Seg Val Ep{epoch}\", unit=\"file\"):\n",
        "            with open(pklf,'rb') as f: batch = pickle.load(f)\n",
        "            items = batch.items() if isinstance(batch,dict) else [(i,e['image']) for i,e in enumerate(batch)]\n",
        "            imgs, msks = [], []\n",
        "            for key, img in items:\n",
        "                arr = img.numpy() if hasattr(img,'numpy') else np.array(img)\n",
        "                imgs.append(seg_tf(Image.fromarray(arr.astype(np.uint8))))\n",
        "                mask = create_mask(ann_dict.get(key,{}).get('bboxes',[]), arr.shape[:2])\n",
        "                msks.append(torch.from_numpy(mask).long())\n",
        "            loader = DataLoader(TensorDataset(torch.stack(imgs), torch.stack(msks)),\n",
        "                                batch_size=seg_bs, shuffle=False,\n",
        "                                num_workers=4, pin_memory=True)\n",
        "            with torch.no_grad():\n",
        "                # Change autocast initialization for older PyTorch versions\n",
        "                #with autocast(enabled=device.type=='cuda'):\n",
        "                with autocast():\n",
        "                    for x,y in loader:\n",
        "                        x,y = x.to(device), y.to(device)\n",
        "                        out = seg_model(x)['out']\n",
        "                        loss = seg_loss(out, y)\n",
        "                        v_loss += loss.item(); v_steps += 1\n",
        "        avg_val = v_loss/v_steps\n",
        "\n",
        "        torch.save(seg_model.state_dict(), f\"deeplab_ep{epoch}.pth\")\n",
        "        print(f\"Seg Ep{epoch} | Train:{avg_tr:.4f} | Val:{avg_val:.4f}\")"
      ],
      "metadata": {
        "id": "1ZCl2bamqwHP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cls(cls_epochs=5, cls_bs=16):\n",
        "    device   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    scaler_c = GradScaler()\n",
        "    cls_model = ResNetCBAM(num_classes=3).to(device)\n",
        "    cls_opt   = torch.optim.Adam(cls_model.parameters(), lr=1e-4)\n",
        "    cls_loss  = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 크기 기반 클래스 분할 경계 (면적 비율 기준)\n",
        "    boundaries = [0.1, 0.5]  # 0~0.1: small, 0.1~0.5: medium, >0.5: large\n",
        "\n",
        "    W = 224  # 출력 폭 고정\n",
        "\n",
        "    # — ROI 추출 및 크기 라벨링 —\n",
        "    rois, labels = [], []\n",
        "    for pklf in tqdm(train_pkls, desc=\"ROI Extract\", unit=\"file\"):\n",
        "        with open(pklf,'rb') as f:\n",
        "            batch = pickle.load(f)\n",
        "        items = batch.items() if isinstance(batch,dict) else [(i,e['image']) for i,e in enumerate(batch)]\n",
        "        for key, img in items:\n",
        "            arr = img.numpy() if hasattr(img,'numpy') else np.array(img)\n",
        "            # 바이너리 마스크 생성\n",
        "            mask_np = create_mask(ann_dict.get(key,{}).get('bboxes',[]), arr.shape[:2]).astype(np.uint8)\n",
        "            cnts,_  = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            if not cnts:\n",
        "                continue\n",
        "            c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "            # 1) 최소 외접 사각형에서 4점 획득\n",
        "            rect = cv2.minAreaRect(c)\n",
        "            box  = cv2.boxPoints(rect)  # shape (4,2)\n",
        "\n",
        "            # 2) 상/하 두 점 분리 후, 밑변이 더 길도록 스왑\n",
        "            pts_sorted = sorted(box, key=lambda p: p[1])\n",
        "            top_pts    = sorted(pts_sorted[:2], key=lambda p: p[0])\n",
        "            bot_pts    = sorted(pts_sorted[2:], key=lambda p: p[0])\n",
        "            top_w = np.linalg.norm(top_pts[1] - top_pts[0])\n",
        "            bot_w = np.linalg.norm(bot_pts[1] - bot_pts[0])\n",
        "            if bot_w < top_w:\n",
        "                top_pts, bot_pts = bot_pts, top_pts\n",
        "                top_w, bot_w   = bot_w, top_w\n",
        "\n",
        "            tl, tr = top_pts\n",
        "            bl, br = bot_pts\n",
        "\n",
        "            src_pts = np.array([tl, tr, br, bl], dtype=np.float32)\n",
        "\n",
        "            # 3) 동적 높이 계산\n",
        "            top_y = min(tl[1], tr[1])\n",
        "            bot_y = max(br[1], bl[1])\n",
        "            h_src = bot_y - top_y\n",
        "            H     = int((h_src / bot_w) * W) if bot_w > 0 else W\n",
        "\n",
        "            dst_pts = np.array([\n",
        "                [0,     0],\n",
        "                [W-1,   0],\n",
        "                [W-1, H-1],\n",
        "                [0,    H-1]\n",
        "            ], dtype=np.float32)\n",
        "\n",
        "            # 4) 투시 변환 및 패딩\n",
        "            M      = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
        "            warped = cv2.warpPerspective(arr[..., ::-1], M, (W, H))\n",
        "            if H < W:\n",
        "                pad_top = (W - H)//2\n",
        "                pad_bot = W - H - pad_top\n",
        "                warped = cv2.copyMakeBorder(warped, pad_top, pad_bot, 0, 0,\n",
        "                                            cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "            # 5) 텐서 변환 및 정규화\n",
        "            roi_tensor = roi_tf(warped).to(device)\n",
        "            rois.append(roi_tensor)\n",
        "\n",
        "            # 6) 면적 비율 기반 크기 라벨링\n",
        "            img_area  = arr.shape[0] * arr.shape[1]\n",
        "            bbox_area = rect[1][0] * rect[1][1]\n",
        "            ratio     = bbox_area / img_area if img_area > 0 else 0\n",
        "            size_label= int(np.digitize(ratio, boundaries))\n",
        "            labels.append(size_label)\n",
        "\n",
        "    # 로더 준비\n",
        "    if not rois:\n",
        "        print(\"No ROIs extracted. Skipping classification.\")\n",
        "        return\n",
        "\n",
        "    X = torch.stack(rois)\n",
        "    Y = torch.tensor(labels, dtype=torch.long)\n",
        "    train_ld = DataLoader(TensorDataset(X, Y),\n",
        "                          batch_size=cls_bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=4,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    # — 분류 훈련 루프 —\n",
        "    for epoch in range(1, cls_epochs+1):\n",
        "        cls_model.train()\n",
        "        total_loss, steps = 0.0, 0\n",
        "        for xb, yb in tqdm(train_ld, desc=f\"Cls Train Ep{epoch}\", unit=\"batch\"):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            with autocast(device_type=device.type):\n",
        "                logits = cls_model(xb)\n",
        "                loss   = cls_loss(logits, yb)\n",
        "            cls_opt.zero_grad()\n",
        "            scaler_c.scale(loss).backward()\n",
        "            scaler_c.step(cls_opt)\n",
        "            scaler_c.update()\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "        print(f\"Cls Ep{epoch} | Train Loss: {total_loss/steps:.4f}\")\n",
        "\n",
        "        # (필요 시 검증 루프 추가)\n",
        "        torch.save(cls_model.state_dict(), f\"resnetcbam_ep{epoch}.pth\")"
      ],
      "metadata": {
        "id": "hYJrlhn0ymSF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KojDeagUrBRe",
        "outputId": "3dcece55-9841-46ac-8f5f-53e90c82793f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-8c099831cb6b>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_s = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Seg Train Ep1:   0%|          | 0/42 [00:00<?, ?file/s]<ipython-input-20-8c099831cb6b>:16: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  with open(pklf,'rb') as f: batch = pickle.load(f)\n",
            "<ipython-input-20-8c099831cb6b>:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Seg Train Ep1: 100%|██████████| 42/42 [24:38<00:00, 35.20s/file]\n",
            "Seg Val Ep1:   0%|          | 0/6 [00:00<?, ?file/s]<ipython-input-20-8c099831cb6b>:45: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  with open(pklf,'rb') as f: batch = pickle.load(f)\n",
            "<ipython-input-20-8c099831cb6b>:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Seg Val Ep1: 100%|██████████| 6/6 [01:39<00:00, 16.59s/file]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seg Ep1 | Train:0.0039 | Val:0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seg Train Ep2:   7%|▋         | 3/42 [01:43<22:39, 34.86s/file]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_5dAnYTrCyp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}