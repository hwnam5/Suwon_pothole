{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, glob, pickle, json, time\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kOQJbFRTq9cT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 모델 정의 —\n",
        "def get_deeplab_model(num_classes: int) -> nn.Module:\n",
        "    model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
        "    model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "    return model\n",
        "\n",
        "class CBAMBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels//reduction, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(channels//reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg = self.fc(self.avg_pool(x))\n",
        "        maxv = self.fc(self.max_pool(x))\n",
        "        attn = self.sigmoid(avg + maxv)\n",
        "        return x * attn\n",
        "\n",
        "class ResNetCBAM(nn.Module):\n",
        "    def __init__(self, num_classes: int = 3):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "        self.backbone.layer1.add_module('cbam1', CBAMBlock(256))\n",
        "        self.backbone.layer2.add_module('cbam2', CBAMBlock(512))\n",
        "        in_feat = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_feat, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ],
      "metadata": {
        "id": "zNhxVug5pgmL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 유틸리티 함수 —\n",
        "def load_annotations(path, retries=3, delay=1):\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            with open(path,'r',encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except OSError:\n",
        "            time.sleep(delay)\n",
        "    raise\n",
        "\n",
        "def create_mask(bboxes, shape):\n",
        "    img = Image.new('L', (shape[1], shape[0]), 0)\n",
        "    d = ImageDraw.Draw(img)\n",
        "    for x,y,w,h in bboxes:\n",
        "        d.rectangle([x,y,x+w,y+h], fill=1)\n",
        "    return np.array(img, dtype=np.int64)\n",
        "\n",
        "def warp_roi(img_t, mask_t, bboxes, size=(224,224)):\n",
        "    mean,std = img_t.new_tensor([0.485,0.456,0.406]).view(3,1,1), img_t.new_tensor([0.229,0.224,0.225]).view(3,1,1)\n",
        "    img = (img_t*std+mean)*255\n",
        "    np_img = img.clamp(0,255).byte().cpu().permute(1,2,0).numpy()[...,::-1]\n",
        "    m = (mask_t.cpu().numpy()>0).astype(np.uint8)\n",
        "    cnts,_ = cv2.findContours(m,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not cnts: return None,[]\n",
        "    c = max(cnts, key=cv2.contourArea)\n",
        "    pts = cv2.approxPolyDP(c,0.02*cv2.arcLength(c,True),True).reshape(-1,2) \\\n",
        "          if len(c)>=4 else cv2.boxPoints(cv2.minAreaRect(c))\n",
        "    s,d = pts.sum(1), np.diff(pts,axis=1).ravel()\n",
        "    tl,br,tr,bl = pts[np.argmin(s)], pts[np.argmax(s)], pts[np.argmin(d)], pts[np.argmax(d)]\n",
        "    src = np.array([tl,tr,br,bl], float)\n",
        "    dst = np.array([[0,0],[size[0]-1,0],[size[0]-1,size[1]-1],[0,size[1]-1]], float)\n",
        "    H = cv2.getPerspectiveTransform(src, dst)\n",
        "    wimg = cv2.warpPerspective(np_img, H, size)\n",
        "    roi = torch.from_numpy(wimg[...,::-1]).permute(2,0,1).float().to(img_t.device)/255\n",
        "    return (roi-mean)/std, []"
      ],
      "metadata": {
        "id": "XdL4-umCq7UX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 경로 설정 및 어노테이션 로드 —\n",
        "DATA_ROOT = '/content/drive/MyDrive/pt_data'\n",
        "AIHUB_ANN = os.path.join(DATA_ROOT, 'aihub_annotations.json')\n",
        "RDD_ANN   = os.path.join(DATA_ROOT, 'rdd2022_train_annotations.json')\n",
        "ann_dict  = {**load_annotations(AIHUB_ANN), **load_annotations(RDD_ANN)}\n",
        "\n",
        "# — 파일 목록 준비 —\n",
        "train_pkls = glob.glob(os.path.join(DATA_ROOT, 'AIhub_Road',    'training_image_batch_*.pkl')) + \\\n",
        "             glob.glob(os.path.join(DATA_ROOT, 'RDD2022',        'training_image_batch_*.pkl'))\n",
        "val_pkls   = glob.glob(os.path.join(DATA_ROOT, 'AIhub_Road',    'validation_image_batch_*.pkl')) + \\\n",
        "             glob.glob(os.path.join(DATA_ROOT, 'RDD2022',        'validation_image_batch_*.pkl'))"
      ],
      "metadata": {
        "id": "-q8qw0Dgq4BB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 트랜스폼 정의 —\n",
        "seg_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "roi_tf = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "1PieIAm9q1eA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 훈련 함수 —\n",
        "def train_seg(seg_epochs=5, seg_bs=16):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    scaler_s = GradScaler()\n",
        "\n",
        "    seg_model = get_deeplab_model(2).to(device)\n",
        "    seg_opt   = torch.optim.Adam(seg_model.parameters(), lr=1e-4)\n",
        "    seg_loss  = nn.CrossEntropyLoss()\n",
        "\n",
        "    # — Segmentation loop —\n",
        "    for epoch in range(1, seg_epochs+1):\n",
        "        # train\n",
        "        seg_model.train()\n",
        "        total_loss, steps = 0, 0\n",
        "        for pklf in tqdm(train_pkls, desc=f\"Seg Train Ep{epoch}\", unit=\"file\"):\n",
        "            with open(pklf,'rb') as f: batch = pickle.load(f)\n",
        "            items = batch.items() if isinstance(batch,dict) else [(i,e['image']) for i,e in enumerate(batch)]\n",
        "            imgs, msks = [], []\n",
        "            for key, img in items:\n",
        "                arr = img.numpy() if hasattr(img,'numpy') else np.array(img)\n",
        "                imgs.append(seg_tf(Image.fromarray(arr.astype(np.uint8))))\n",
        "                mask = create_mask(ann_dict.get(key,{}).get('bboxes',[]), arr.shape[:2])\n",
        "                msks.append(torch.from_numpy(mask).long())\n",
        "            loader = DataLoader(TensorDataset(torch.stack(imgs), torch.stack(msks)),\n",
        "                                batch_size=seg_bs, shuffle=True,\n",
        "                                num_workers=4, pin_memory=True)\n",
        "            for x,y in loader:\n",
        "                x,y = x.to(device), y.to(device)\n",
        "                # Change autocast initialization for older PyTorch versions\n",
        "                #with autocast(enabled=device.type=='cuda'):\n",
        "                with autocast():\n",
        "                    out = seg_model(x)['out']\n",
        "                    loss = seg_loss(out, y)\n",
        "                seg_opt.zero_grad()\n",
        "                scaler_s.scale(loss).backward()\n",
        "                scaler_s.step(seg_opt)\n",
        "                scaler_s.update()\n",
        "                total_loss += loss.item(); steps += 1\n",
        "        avg_tr = total_loss/steps\n",
        "\n",
        "        # val\n",
        "        seg_model.eval()\n",
        "        v_loss, v_steps = 0, 0\n",
        "        for pklf in tqdm(val_pkls, desc=f\"Seg Val Ep{epoch}\", unit=\"file\"):\n",
        "            with open(pklf,'rb') as f: batch = pickle.load(f)\n",
        "            items = batch.items() if isinstance(batch,dict) else [(i,e['image']) for i,e in enumerate(batch)]\n",
        "            imgs, msks = [], []\n",
        "            for key, img in items:\n",
        "                arr = img.numpy() if hasattr(img,'numpy') else np.array(img)\n",
        "                imgs.append(seg_tf(Image.fromarray(arr.astype(np.uint8))))\n",
        "                mask = create_mask(ann_dict.get(key,{}).get('bboxes',[]), arr.shape[:2])\n",
        "                msks.append(torch.from_numpy(mask).long())\n",
        "            loader = DataLoader(TensorDataset(torch.stack(imgs), torch.stack(msks)),\n",
        "                                batch_size=seg_bs, shuffle=False,\n",
        "                                num_workers=4, pin_memory=True)\n",
        "            with torch.no_grad():\n",
        "                # Change autocast initialization for older PyTorch versions\n",
        "                #with autocast(enabled=device.type=='cuda'):\n",
        "                with autocast():\n",
        "                    for x,y in loader:\n",
        "                        x,y = x.to(device), y.to(device)\n",
        "                        out = seg_model(x)['out']\n",
        "                        loss = seg_loss(out, y)\n",
        "                        v_loss += loss.item(); v_steps += 1\n",
        "        avg_val = v_loss/v_steps\n",
        "\n",
        "        torch.save(seg_model.state_dict(), f\"deeplab_ep{epoch}.pth\")\n",
        "        print(f\"Seg Ep{epoch} | Train:{avg_tr:.4f} | Val:{avg_val:.4f}\")"
      ],
      "metadata": {
        "id": "1ZCl2bamqwHP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cls(cls_epochs=5, cls_bs=16):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    scaler_c = GradScaler()\n",
        "    cls_model = ResNetCBAM(num_classes=3).to(device)\n",
        "    cls_opt   = torch.optim.Adam(cls_model.parameters(), lr=1e-4)\n",
        "    cls_loss  = nn.CrossEntropyLoss()\n",
        "\n",
        "  # — ROI 추출 —\n",
        "    rois, labels = [], []\n",
        "    for pklf in tqdm(train_pkls, desc=\"ROI Extract\", unit=\"file\"):\n",
        "        with open(pklf,'rb') as f: batch = pickle.load(f)\n",
        "        items = batch.items() if isinstance(batch,dict) else [(i,e['image']) for i,e in enumerate(batch)]\n",
        "        for key, img in items:\n",
        "            arr = img.numpy() if hasattr(img,'numpy') else np.array(img)\n",
        "            mask = torch.from_numpy(create_mask(ann_dict.get(key,{}).get('bboxes',[]), arr.shape[:2]))\n",
        "            # Ensure warp_roi can handle tensor inputs and potentially move to device\n",
        "            roi, _ = warp_roi(seg_tf(Image.fromarray(arr.astype(np.uint8))).to(device), mask.to(device), [])\n",
        "            if roi is not None:\n",
        "                rois.append(roi.cpu()); labels.append(int(ann_dict.get(key,{}).get('labels',[0])[0]))\n",
        "\n",
        "    # Handle case where no ROIs are extracted\n",
        "    if not rois:\n",
        "        print(\"No ROIs extracted for training. Skipping classification training.\")\n",
        "        train_ld = None # Or handle this differently based on desired behavior\n",
        "    else:\n",
        "        X = torch.stack([roi_tf(r) for r in rois])\n",
        "        Y = torch.tensor(labels)\n",
        "        train_ld = DataLoader(TensorDataset(X, Y), batch_size=cls_bs, shuffle=True,\n",
        "                              num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Prepare val set ROI in the same way...\n",
        "    val_rois, val_labels = [], []\n",
        "    for pklf in tqdm(val_pkls, desc=\"Val ROI Extract\", unit=\"file\"):\n",
        "         with open(pklf,'rb') as f: batch = pickle.load(f)\n",
        "         items = batch.items() if isinstance(batch,dict) else [(i,e['image']) for i,e in enumerate(batch)]\n",
        "         for key, img in items:\n",
        "             arr = img.numpy() if hasattr(img,'numpy') else np.array(img)\n",
        "             mask = torch.from_numpy(create_mask(ann_dict.get(key,{}).get('bboxes',[]), arr.shape[:2]))\n",
        "             roi, _ = warp_roi(seg_tf(Image.fromarray(arr.astype(np.uint8))).to(device), mask.to(device), [])\n",
        "             if roi is not None:\n",
        "                 val_rois.append(roi.cpu()); val_labels.append(int(ann_dict.get(key,{}).get('labels',[0])[0]))\n",
        "\n",
        "    # Handle case where no validation ROIs are extracted\n",
        "    if not val_rois:\n",
        "        print(\"No ROIs extracted for validation. Skipping classification validation.\")\n",
        "        val_ld = None # Or handle this differently based on desired behavior\n",
        "    else:\n",
        "        val_X = torch.stack([roi_tf(r) for r in val_rois])\n",
        "        val_Y = torch.tensor(val_labels)\n",
        "        val_ld = DataLoader(TensorDataset(val_X, val_Y), batch_size=cls_bs, shuffle=False,\n",
        "                            num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "    # — Classification loop —\n",
        "    if train_ld is not None and val_ld is not None: # Only run if both loaders are prepared\n",
        "        for epoch in range(1, cls_epochs+1):\n",
        "            cls_model.train()\n",
        "            total_loss, steps = 0, 0\n",
        "            for xb,yb in tqdm(train_ld, desc=f\"Cls Train Ep{epoch}\", unit=\"batch\"):\n",
        "                xb,yb = xb.to(device), yb.to(device)\n",
        "                # Change autocast initialization for older PyTorch versions\n",
        "                with autocast():\n",
        "                #with autocast(enabled=device.type=='cuda'):\n",
        "                    logits = cls_model(xb)\n",
        "                    loss = cls_loss(logits, yb)\n",
        "                cls_opt.zero_grad()\n",
        "                scaler_c.scale(loss).backward()\n",
        "                scaler_c.step(cls_opt)\n",
        "                scaler_c.update()\n",
        "                total_loss += loss.item(); steps += 1\n",
        "            avg_tr = total_loss/steps\n",
        "\n",
        "            cls_model.eval()\n",
        "            v_loss, v_steps = 0, 0\n",
        "            for xb,yb in tqdm(val_ld, desc=f\"Cls Val Ep{epoch}\", unit=\"batch\"):\n",
        "                xb,yb = xb.to(device), yb.to(device)\n",
        "                with torch.no_grad():\n",
        "                     # Change autocast initialization for older PyTorch versions\n",
        "                    with autocast(enabled=device.type=='cuda'):\n",
        "                        logits = cls_model(xb)\n",
        "                        loss = cls_loss(logits, yb)\n",
        "                        v_loss += loss.item(); v_steps += 1\n",
        "            avg_val = v_loss/v_steps\n",
        "\n",
        "            torch.save(cls_model.state_dict(), f\"resnetcbam_ep{epoch}.pth\")\n",
        "            print(f\"Cls Ep{epoch} | Train:{avg_tr:.4f} | Val:{avg_val:.4f}\")\n",
        "    else:\n",
        "        print(\"Skipping classification training and validation due to no extracted ROIs.\")"
      ],
      "metadata": {
        "id": "hYJrlhn0ymSF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KojDeagUrBRe",
        "outputId": "3dcece55-9841-46ac-8f5f-53e90c82793f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-8c099831cb6b>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_s = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Seg Train Ep1:   0%|          | 0/42 [00:00<?, ?file/s]<ipython-input-20-8c099831cb6b>:16: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  with open(pklf,'rb') as f: batch = pickle.load(f)\n",
            "<ipython-input-20-8c099831cb6b>:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Seg Train Ep1:   2%|▏         | 1/42 [00:32<21:52, 32.01s/file]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_5dAnYTrCyp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}