{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Q0c3tiUzp3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598c3826-3463-4b20-cc5f-e08f59ed8b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=973d90742ccdb63f21c95b2d1405cbdde4607c9b874ac0455134aaa528064d47\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/0c/b3/674aea8c5d91c642c817d4d630bd58faa316724b136844094d\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "!pip install utils\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from torch import einsum\n",
        "import cv2\n",
        "import scipy.misc\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYnrXXeIBohF",
        "outputId": "fb57c64d-2e5b-4944-980d-1065a55a9b8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/data/V_resize_224.pkl', 'rb') as f:\n",
        "     resize_224 = pickle.load(f)\n",
        "     print('V resize 224 data : ', len(resize_224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEcAYokYCQSp",
        "outputId": "f7c048f7-bb74-4b8d-ef57-31c52e4fdad4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V resize 224 data :  4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "for file_name, data in resize_224.items():\n",
        "    X.append(data[\"matrix\"])\n",
        "    y.append(data[\"pothole\"])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "JcS5RmLrETxM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n",
        "print(\"Testing set size:\", len(X_test))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB-_sVXfEoah",
        "outputId": "357f20a3-9d92-4b10-bf3a-f71baf020d11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 3200\n",
            "Validation set size: 400\n",
            "Testing set size: 400\n",
            "(3200, 224, 224)\n",
            "(400, 224, 224)\n",
            "(400, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: change X_train, X_val, X_test to 1 channel\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=1)\n",
        "X_val = np.expand_dims(X_val, axis=1)\n",
        "X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "y_train = np.expand_dims(y_train, axis=1)\n",
        "y_val = np.expand_dims(y_val, axis=1)\n",
        "y_test = np.expand_dims(y_test, axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7HtaXt6GMxW",
        "outputId": "ea04f8ed-269c-4916-dcc8-76c06cc8aa6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3200, 1, 1, 224, 224)\n",
            "(400, 1, 1, 224, 224)\n",
            "(400, 1, 1, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: permute the train/val/test to have channel in front\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BFuVaUbKkUD",
        "outputId": "cec14ca8-c263-4126-cfba-4f7521204f41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3200, 1, 1, 224, 224)\n",
            "(400, 1, 1, 224, 224)\n",
            "(400, 1, 1, 224, 224)\n",
            "(3200, 1)\n",
            "(400, 1)\n",
            "(400, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
        "val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "1hGcNLFqFrwc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class ResNet18Binary(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNet18Binary, self).__init__()\n",
        "\n",
        "        self.resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "        # Modify the first convolutional layer to accept 1-channel input\n",
        "        self.resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Modify the last fully connected layer for binary classification\n",
        "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)"
      ],
      "metadata": {
        "id": "-VeSqsFAkrnv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18Binary()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "WuTPXoz9lKyS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []"
      ],
      "metadata": {
        "id": "05Qvd6_TpAKI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        # Remove the extra dimension using squeeze\n",
        "        inputs = inputs.squeeze(2)  # Remove dimension at index 2\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.squeeze(1)) # Remove extra dimension from labels\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += (predicted == labels.squeeze(1)).sum().item() # Remove extra dimension from labels\n",
        "        train_losses.append(loss.item())\n",
        "        train_accuracies.append(100 * train_correct / train_total)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            # Remove the extra dimension using squeeze\n",
        "            inputs = inputs.squeeze(2)  # Remove dimension at index 2\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.squeeze(1))\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels.squeeze(1)).sum().item() # Remove extra dimension from labels\n",
        "            val_losses.append(loss.item())\n",
        "            val_accuracies.append(100 * val_correct / val_total)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {train_loss / len(train_loader):.4f}, Train Acc: {100 * train_correct / train_total:.2f}%, '\n",
        "          f'Val Loss: {val_loss / len(val_loader):.4f}, Val Acc: {100 * val_correct / val_total:.2f}%')\n",
        "\n",
        "# Testing loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        # Remove the extra dimension using squeeze\n",
        "        inputs = inputs.squeeze(2)  # Remove dimension at index 2\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.squeeze(1)).sum().item()  # Remove extra dimension from labels\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "WVjXabP1TcQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98707be-8371-4e3e-a3fe-a434f43cacc9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Train Loss: 0.6006, Train Acc: 68.50%, Val Loss: 0.7579, Val Acc: 74.00%\n",
            "Epoch [2/30], Train Loss: 0.4854, Train Acc: 76.78%, Val Loss: 0.5796, Val Acc: 64.75%\n",
            "Epoch [3/30], Train Loss: 0.4326, Train Acc: 79.50%, Val Loss: 0.4284, Val Acc: 79.25%\n",
            "Epoch [4/30], Train Loss: 0.3442, Train Acc: 84.97%, Val Loss: 0.3912, Val Acc: 82.50%\n",
            "Epoch [5/30], Train Loss: 0.3060, Train Acc: 86.31%, Val Loss: 0.4004, Val Acc: 83.75%\n",
            "Epoch [6/30], Train Loss: 0.2787, Train Acc: 88.19%, Val Loss: 0.3206, Val Acc: 88.75%\n",
            "Epoch [7/30], Train Loss: 0.2303, Train Acc: 90.56%, Val Loss: 0.3148, Val Acc: 89.25%\n",
            "Epoch [8/30], Train Loss: 0.1863, Train Acc: 92.44%, Val Loss: 0.3483, Val Acc: 86.75%\n",
            "Epoch [9/30], Train Loss: 0.1751, Train Acc: 92.97%, Val Loss: 0.4277, Val Acc: 85.00%\n",
            "Epoch [10/30], Train Loss: 0.1555, Train Acc: 93.53%, Val Loss: 0.3215, Val Acc: 88.00%\n",
            "Epoch [11/30], Train Loss: 0.1489, Train Acc: 94.25%, Val Loss: 0.3424, Val Acc: 88.25%\n",
            "Epoch [12/30], Train Loss: 0.1272, Train Acc: 94.69%, Val Loss: 0.3077, Val Acc: 90.75%\n",
            "Epoch [13/30], Train Loss: 0.1079, Train Acc: 96.25%, Val Loss: 0.4000, Val Acc: 87.00%\n",
            "Epoch [14/30], Train Loss: 0.1023, Train Acc: 96.38%, Val Loss: 0.4406, Val Acc: 88.00%\n",
            "Epoch [15/30], Train Loss: 0.0703, Train Acc: 97.38%, Val Loss: 0.3648, Val Acc: 90.00%\n",
            "Epoch [16/30], Train Loss: 0.0589, Train Acc: 98.16%, Val Loss: 0.3178, Val Acc: 91.75%\n",
            "Epoch [17/30], Train Loss: 0.1033, Train Acc: 96.62%, Val Loss: 0.3339, Val Acc: 89.50%\n",
            "Epoch [18/30], Train Loss: 0.0374, Train Acc: 98.75%, Val Loss: 0.4132, Val Acc: 90.75%\n",
            "Epoch [19/30], Train Loss: 0.0243, Train Acc: 99.19%, Val Loss: 0.4573, Val Acc: 89.00%\n",
            "Epoch [20/30], Train Loss: 0.1267, Train Acc: 95.31%, Val Loss: 0.2991, Val Acc: 90.75%\n",
            "Epoch [21/30], Train Loss: 0.0393, Train Acc: 98.78%, Val Loss: 0.3905, Val Acc: 91.50%\n",
            "Epoch [22/30], Train Loss: 0.0422, Train Acc: 98.69%, Val Loss: 0.3994, Val Acc: 91.25%\n",
            "Epoch [23/30], Train Loss: 0.0774, Train Acc: 97.28%, Val Loss: 0.3591, Val Acc: 90.25%\n",
            "Epoch [24/30], Train Loss: 0.0406, Train Acc: 98.78%, Val Loss: 0.3264, Val Acc: 91.25%\n",
            "Epoch [25/30], Train Loss: 0.0166, Train Acc: 99.47%, Val Loss: 0.4867, Val Acc: 90.50%\n",
            "Epoch [26/30], Train Loss: 0.0551, Train Acc: 98.44%, Val Loss: 0.4702, Val Acc: 87.00%\n",
            "Epoch [27/30], Train Loss: 0.0314, Train Acc: 98.91%, Val Loss: 0.5071, Val Acc: 90.50%\n",
            "Epoch [28/30], Train Loss: 0.0480, Train Acc: 98.34%, Val Loss: 0.5745, Val Acc: 89.00%\n",
            "Epoch [29/30], Train Loss: 0.0722, Train Acc: 97.44%, Val Loss: 0.3951, Val Acc: 90.00%\n",
            "Epoch [30/30], Train Loss: 0.0349, Train Acc: 98.72%, Val Loss: 0.3616, Val Acc: 91.00%\n",
            "Test Accuracy: 92.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAfFEr7Wo3TY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}